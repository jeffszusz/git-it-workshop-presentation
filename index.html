<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0' />
    <title>Apache &amp; Nginx at Scale</title>
    <link href='big.css' rel='stylesheet' type='text/css' />
    <link href='highlight.css' rel='stylesheet' type='text/css' />
    <style>
      .new-shiny { background: #aaaaaa; }
      .title-page {
      }
      .title-page img {
        max-width: 1em;
        max-height: 1em;
        position: relative;
        top: 0.15em;
      }
      .apache-logo {
        left: 0.10em;
      }
      .center-image div {
        background-position: center top;
        background-size: 60%;
        background-repeat: no-repeat;
      }
      table {
        border-spacing: 0.5em 0.25em;
      }
      pre {
        margin:0;
        padding:0.2em;
        background:#fff;
        color:#000;
        font-weight:normal;
      }
      .dark pre em {
        color: #d13787;
      }
      .light pre em {
        color: #fadb03;
      }

    </style>
    <script src='big.js'></script>
    <script src='highlight.js'></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body class='dark'>
    <div data-bodyclass="title-page">
      <img class="apache-logo" src="images/apache-logo.png" alt="apache logo" /> Apache &amp;
      <br /><img src="images/nginx-logo.png" alt="nginx logo" /> NGiNX
      <br /><span style="display: inline-block; width: 1em; text-align: center;">@</span> Scale
    </div>
    <div>
      <h1>@jeffszusz</h1>
      <ul>
        <li>full-stack API and SPA developer</li>
        <li>accidental DevOps engineer</li>
      </ul>
    </div>
    <div>
      What's this talk really about?
    </div>
    <div>
      Apache fails as a proxy server at web scale, and I'm going to tell you why.
      <notes>
        I'm not going to show you Apache configuration files or NGiNX configuration files,
        I'm not going to give you an exhaustive feature comparison
        or look at any source code for either web server.
      </notes>
    </div>
    <div>
      45% of the web runs on Apache.
      <notes>
        41% of the million busiest websites run on Apache.
      </notes>
    </div>
    <div>
      19% of the web runs on NGiNX.
      <notes>
        28% of the million busiest websites run on NGiNX.
        That means the engineers behind some of the hottest stuff are choosing NGiNX more than the average person.
      </notes>
    </div>
    <div>
      Apache and Microsoft once held a nearly 60/40 split.
      <notes>So why did NGiNX cut such a huge swath?</notes>
    </div>
    <div>
      c10k
      <notes>
        10k current connections used to be a problem even for the top tier software companies.
        For them, the numbers have gotten larger:
      </notes>
    </div>
    <div>
      WhatsApp: 2 Million concurrent connections on a 1U server (100GB RAM, 24 CPU cores)
      <notes>
        This was written in Erlang and Facebook was willing to pay 19 billion dollars for it.
      </notes>
    </div>
    <div>
      MigratoryData: 12 Million concurrent connections on a single 1U server (55GB RAM, 24 CPU cores)
      <notes>
        Throughput like this requires writing custom systems to take networking,
        memory and CPU management away from the Unix operating system that's hosting the application.
      </notes>
    </div>
    <div>
      On commodity hardware, even just 10k concurrent connections can be a tall order.
      <notes>And we're unlikely to write kernel extensions or a cusotm network stack</notes>
    </div>
    <div>
      Dreamhost did some benchmarks:
      <ul>
        <li>8 tests, slowly increasing the number of concurrent connections</li>
        <li>each of the 8 tests made a total of 25,000 requests for a 5k PNG file</li>
      </ul>
      <notes>
        [server]$ ab -n 25000 -c 50 http://www.example.com/dreamhost_logo.png
      </notes>
    </div>
    <div data-background-image="images/memory_usage_graph.jpg" data-bodyclass="center-image">
    </div>
    <div data-background-image="images/requests_per_second_graph.jpg" data-bodyclass="center-image">
    </div>
    <div>
      NGiNX clearly wins in raw performance
    </div>
    <div>
      But Apache is everywhere! It's familiar!
      <notes>
        Why move to a totally different webserver that's unfamiliar and maybe doesn't come with your web hosting?
      </notes>
      <notes>
        Multiple Apache instances behind a load balancer can solve all our problems, right?
      </notes>
    </div>
    <div>
      Apache can be used as an HTTP proxy server for microservices.
      <notes>what's a proxy? what's a microservice?</notes>
      <notes>In fact, that's what I've been using it for. But one day, it blew up.</notes>
    </div>
    <div>
      <pre>
[proxy_balancer:emerg] (28)<em>No space left on device</em>:
AH01180: <em>mutex creation</em> of proxy-balancer-shm :
p75dfed5c_myservice <em>failed</em></pre>
    </div>
    <div>
      WTF?
    </div>
    <div data-bodyclass="nobreak">
      Semaphores: shared memory structures used for communicating between processes
      <notes>
        Semaphore Arrays, which Apache uses, are more sophisticated than standard semaphores: each can manage multiple resources
      </notes>
    </div>
    <div>
      RHEL / CentOS / Fedora default to 128 Semaphore Arrays
      <notes>
        Apache uses 2 Semaphore Arrays to manage its processes.
      </notes>
      <notes>
        Even with Apache's most sophisticated multiprocessing module, each proxy definition soaks up an entire Semaphore Array.
      </notes>
      <notes>
        So if you have a microservice for book title info, that's one proxy definition even if you have 20 of them
        But you might have another service for prices, another for inventory, services for checkout and returns and warehouse location.
      </notes>
    </div>
    <div>
      If you're proxying to 127 distinct microservices, Apache won't start.
      <notes>
        If you're dealing with that many services, you probably register them dynamically.
      </notes>
    </div>
    <div>
      Imagine the 127th service comes online and silently adds itself to the Apache configuration.
    </div>
    <div>
      <span style="color:orange">boom</span>
    </div>
    <div>
      Solution?
    </div>
    <div>
      <pre>sysctl -w kernel.sem="250 64000 32 <em>256</em>"</pre>
    </div>
    <div>
      Problem solved... until a 255th service silently registers itself.
    </div>
    <div>
      And Apache is now operating outside of intended parameters.
    </div>
    <div>
      We bought ourselves some time, but we'll have to come back to this later.
      <notes>For now, we can carry on writing features in our API until...</notes>
    </div>
    <div>
      5 Minutes of downtime across the entire system during peak traffic.
      <notes>remember a load-balanced Apache cluster is directing traffic for our entire system.</notes>
    </div>
    <div>
      Apache instances don't seem strained for resources.
    </div>
    <div>
      WTF?
    </div>
    <div>
      <pre>
[mpm_event:error]
  [pid 23971:tid 140569643808896] AH00485:
  <em>scoreboard is full, not at MaxRequestWorkers</em></pre>
      <notes>
        During a "graceful" restart, Apache signals all its child processes to finish their work and terminate, while it creates new processes for new connections using the new configs.
      </notes>
      <notes>
        Under a large load of long-running processes, the "scoreboard" that keeps track of work being done can be occupied by old processes while new ones try to fork.
      </notes>
    </div>
    <div>
      If it's handling a lot of long-running traffic, adding and removing services causes Apache to slow down
      <notes>significantly, for anywhere between 5 seconds and a full minute</notes>
    </div>
    <div>
      Facepalm
      <notes>Apache was monitored and catalogued by the dynamic service discovery system.</notes>
      <notes>If Apache doesn't respond to a health check every 5 seconds, the catalog updates. Which causes every instance in the Apache cluster to reload.</notes>
    </div>
    <div>
      The <em>full scoreboard</em> error cascades across the entire cluster, in turn causing each instance to flutter.
    </div>
    <div>
      It settles down in ~5 minutes.
      <notes>
        Investigation shows it's been doing this roughly every 150 hours (just over six days) for a while, but the fluttering didn't last long enough to catch in monitoring because load has been lower.
      </notes>
      <notes>
        Turns out some other service in the catalog has a memory leak: every six days, under the usual load, it reaches max memory and recovers itself. There are no negative effects... except for a catalog reload in apache.
      </notes>
    </div>
    <div>Solution?</div>
    <div>
      The fluttering can be fixed by making sure Apache itself doesn't trigger catalog reloads.
    </div>
    <div>
      To fix the <em>full scoreboard</em> issue that causes a performance drop when services are added or removed, we need to look elsewhere.
      <notes>
        A newer version of Apache makes this error happen far less frequently and under much higher stress, but it doesn't make the problem go away.
      </notes>
    </div>
    <div>
      enter NGiNX
    </div>
    <div>
      NGiNX was started in 2002 as a solution to the C10K problem, and released in 2004.
    </div>
    <div>
      Unlike Apache, NGiNX was built with high concurrency in mind.
      <notes>Where apache ties work to processes or threads, NGiNX does not.</notes>
    </div>
    <div>
      NGiNX spawns a few processes, each of which contains an event loop.
      <notes>waitstaff at a restaurant analogy!</notes>
      <notes>ideally one per processor core on the server.</notes>
      <notes>
        Each process can handle thousands of connections without blocking or spawning any new processes or threads.
      </notes>
      <notes>
        This keeps memory and CPU usage very low, prevents the need for an abundance of
        system Semaphores, and allows for smooth configuration reloads - dynamic addition and removal of upstream servers.
      </notes>
    </div>
    <div>
      Apache can spawn 400 concurrent connections by default
      <notes>(16 processes and 25 threads per process)</notes>
      <notes>that's with its most modern multiprocessing module, mpm_event</notes>
    </div>
    <div>
      Maximum possible is 32,000 concurrent connections
      <notes>(16 processes with 2000 threads per process)</notes>
      <notes>The kind of hardware you'd need to run it on to ever reach that is insanely expensive,</notes>
      <notes>to exceed that you'd need to edit the Apache source and compile it yourself.</notes>
    </div>
    <div>
      NGiNX can handle ~65,500 concurrent connections by default
      <notes>assuming your Linux OS is tuned appropriately and you have the memory and CPU to support it</notes>
      <notes>limited by file descriptors (around 65.5k by default) and available TCP sockets on the machine (also 65.5k per network interface / IP address)</notes>
    </div>
    <div>
      NGiNX can handle more traffic,
      <ul>
        <li>has almost no impact from reloads of its proxy catalog,</li>
        <li>has no limit to the number of distinct services it can proxy to,</li>
        <li>and uses fewer system resources than Apache.</li>
      </ul>
    </div>
    <div>
      Compared to NGiNX, Apache just isn't cut out for <em>"Web Scale"</em>
      <notes>Apache's still fantastic for most use cases, and you can solve traffic
        problems by throwing more Apache instances behind a more powerful load balancer
        though more servers means a higher hosting cost.</notes>
    </div>
    <div>
      Jeff Szusz
      <br />Bull Rush Studios Inc.
      <br />
      <br />twitter:
      <br />@jeffszusz for games, food, etc.
      <br />@BullRushStudios for development
      <br />
      <br />hackforge email: jeff@hackf.org
    </div>
  </body>
</html>
